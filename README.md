# RAG-система 
## Обзор проекта

Проект представляет собой end-to-end RAG-систему для ответов на вопросы научного NLP, основанную на биомедицинской литературе.

## Архитектура
1. Retrieval 

* Используется модель sentence-transformers/all-MiniLM-L6-v2

* Вычисление эмбеддингов с использованием PyTorch

* Векторный индекс FAISS

* Поиск релевантных текстовых чанков с метаданными (PMID, заголовок)

2. Generation 

* LLM через OpenRouter API

* Prompt engineering для научного стиля с опорой на контекст

* Ограничение генерации только извлечённым контекстом (без “галлюцинаций”)

* Поддержка ссылок на источники

3. Pipeline

* Модульный пайплайн для соединения retrieval и generation

* Лёгко расширяемый для бенчмарков, оценки или замены моделей

4. UI & Визуализация

* Streamlit-приложение для интерактивных запросов

* Отображение ответа и источников

## Используемые технологии

* **PyTorch, HuggingFace (sentence-transformers)**
* **FAISS for vector search**
* **Requests for API calls**
* **Streamlit**
* **LLM APIs (OpenRouter)**

## Как запустить 

pip install -r requirements.txt

In app/main.py:OPENROUTER_API_KEY = "your_openrouter_api_key"

streamlit run app/main.py

## Ответы на вопросы

1. На какие модальности данных можно расширить решение?

Текст: расширение за пределы PubMed (клинические отчёты, патенты, новости о лекарствах).

Структурированные данные: базы данных биомаркеров, результаты клинических исследований.

Изображения: медицинские снимки (MRI, PET) с помощью мульти-модальных моделей.

Временные ряды: данные о прогрессии болезни, когнитивные тесты, биометрические показатели.

2. Как это можно сделать?

Мультимодальные embeddings: объединение текстовых и визуальных представлений в общую векторную пространство (например, CLIP или MedCLIP для изображений).

Модификация pipeline: добавить новые retrieval-компоненты под каждую модальность.

Объединение контекста: генератор LLM будет использовать мульти-модальные контексты для ответа.

Фильтрация и согласование источников: чтобы сохранить точность и цитируемость при разных типах данных.

3. Какие модели и почему выбрали для решения?

Sentence-Transformers / MiniLM – для текстовых embeddings: быстро, компактно, хорошо подходит для semantic search.

FAISS – для эффективного поиска релевантных chunks.

OpenRouter / GPT-4o-mini – генерация научно структурированных ответов, гибкость prompt-инженеринга, поддержка source citation.

Streamlit – интерактивная визуализация и демонстрация прототипа.

* Обоснование выбора:

Баланс между точностью и скоростью – MiniLM позволяет обрабатывать сотни текстов без сильных требований к ресурсам.

Модульность – легко заменять LLM или embeddings-модель.

Научная достоверность – генератор строго ограничен контекстом, минимизация галлюцинаций.
