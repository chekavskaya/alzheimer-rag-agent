{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19591,
     "status": "ok",
     "timestamp": 1770479798644,
     "user": {
      "displayName": "Юлия Федурова",
      "userId": "06389464721626605031"
     },
     "user_tz": -180
    },
    "id": "lo_e4y-2U0FP",
    "outputId": "36337d04-6dd0-4df1-aebb-66e7cce876eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28037,
     "status": "ok",
     "timestamp": 1770480254240,
     "user": {
      "displayName": "Юлия Федурова",
      "userId": "06389464721626605031"
     },
     "user_tz": -180
    },
    "id": "sjvvqVoRUig9",
    "outputId": "c090726d-feed-42d2-ffc4-e3a968438374"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  3.90it/s]\n",
      "100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 85 chunks for RAG modeling\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# ============================================================\n",
    "# 1. PubMed Search (Abstracts + Metadata)\n",
    "# ============================================================\n",
    "\n",
    "PUBMED_SEARCH_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "PUBMED_FETCH_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "QUERY = \"Alzheimer disease therapeutic targets\"\n",
    "MAX_RESULTS = 30\n",
    "\n",
    "params = {\n",
    "    \"db\": \"pubmed\",\n",
    "    \"term\": QUERY,\n",
    "    \"retmax\": MAX_RESULTS,\n",
    "    \"retmode\": \"json\"\n",
    "}\n",
    "\n",
    "search_resp = requests.get(PUBMED_SEARCH_URL, params=params)\n",
    "ids = search_resp.json()[\"esearchresult\"][\"idlist\"]\n",
    "\n",
    "articles = []\n",
    "\n",
    "for pmid in tqdm(ids):\n",
    "    fetch_params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": pmid,\n",
    "        \"retmode\": \"xml\"\n",
    "    }\n",
    "\n",
    "    xml = requests.get(PUBMED_FETCH_URL, params=fetch_params).text\n",
    "    soup = BeautifulSoup(xml, \"xml\")\n",
    "\n",
    "    abstract = soup.find(\"AbstractText\")\n",
    "    title = soup.find(\"ArticleTitle\")\n",
    "    pmcid_tag = soup.find(\"ArticleId\", {\"IdType\": \"pmc\"})\n",
    "\n",
    "    articles.append({\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid_tag.text if pmcid_tag else None,\n",
    "        \"title\": title.text if title else \"\",\n",
    "        \"abstract\": abstract.text if abstract else \"\"\n",
    "    })\n",
    "\n",
    "pubmed_df = pd.DataFrame(articles)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Europe PMC Full-Text Retrieval (Introduction + Conclusion)\n",
    "# ============================================================\n",
    "\n",
    "def fetch_full_text(pmcid):\n",
    "    if not pmcid:\n",
    "        return None\n",
    "    url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/{pmcid}/fullTextXML\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def extract_sections(xml_text):\n",
    "    if not xml_text:\n",
    "        return {\"introduction\": \"\", \"conclusion\": \"\"}\n",
    "\n",
    "    soup = BeautifulSoup(xml_text, \"xml\")\n",
    "    intro, concl = \"\", \"\"\n",
    "\n",
    "    for sec in soup.find_all(\"sec\"):\n",
    "        title = sec.title.text.lower() if sec.title else \"\"\n",
    "        text = \" \".join(p.text for p in sec.find_all(\"p\"))\n",
    "\n",
    "        if \"introduction\" in title:\n",
    "            intro += \" \" + text\n",
    "\n",
    "        if \"conclusion\" in title or \"discussion\" in title:\n",
    "            concl += \" \" + text\n",
    "\n",
    "    return {\n",
    "        \"introduction\": intro.strip(),\n",
    "        \"conclusion\": concl.strip()\n",
    "    }\n",
    "\n",
    "\n",
    "introductions = []\n",
    "conclusions = []\n",
    "\n",
    "for pmcid in tqdm(pubmed_df.pmcid):\n",
    "    xml = fetch_full_text(pmcid)\n",
    "    sections = extract_sections(xml)\n",
    "    introductions.append(sections[\"introduction\"])\n",
    "    conclusions.append(sections[\"conclusion\"])\n",
    "\n",
    "pubmed_df[\"introduction\"] = introductions\n",
    "pubmed_df[\"conclusion\"] = conclusions\n",
    "\n",
    "# ============================================================\n",
    "# 3. Text Cleaning\n",
    "# ============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "for col in [\"abstract\", \"introduction\", \"conclusion\"]:\n",
    "    pubmed_df[col] = pubmed_df[col].astype(str).apply(clean_text)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Combine Sections (Assignment Requirement)\n",
    "# ============================================================\n",
    "\n",
    "def combine_sections(row):\n",
    "    parts = []\n",
    "    if row.abstract:\n",
    "        parts.append(\"ABSTRACT: \" + row.abstract)\n",
    "    if row.introduction:\n",
    "        parts.append(\"INTRODUCTION: \" + row.introduction)\n",
    "    if row.conclusion:\n",
    "        parts.append(\"CONCLUSION: \" + row.conclusion)\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "pubmed_df[\"full_text\"] = pubmed_df.apply(combine_sections, axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Exploratory Data Analysis (Light)\n",
    "# ============================================================\n",
    "\n",
    "pubmed_df[\"text_length\"] = pubmed_df.full_text.apply(len)\n",
    "pubmed_df[[\"text_length\"]].describe()\n",
    "\n",
    "# ============================================================\n",
    "# 6. Chunking for RAG (No LangChain)\n",
    "# ============================================================\n",
    "\n",
    "def split_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, row in pubmed_df.iterrows():\n",
    "    chunks = split_text(row.full_text)\n",
    "    for ch in chunks:\n",
    "        rows.append({\n",
    "            \"text\": ch,\n",
    "            \"pmid\": row.pmid,\n",
    "            \"title\": row.title\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# 7. Save Prepared Dataset\n",
    "# ============================================================\n",
    "\n",
    "chunks_df.to_csv(\"/content/drive/MyDrive/data/pubmed_chunks.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(chunks_df)} chunks for RAG modeling\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMlw/UGCv1csiGI71tw4sj2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
